{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77c31c0d-3a18-44eb-bdf9-0a049495aa87",
   "metadata": {},
   "source": [
    "# Neural Collabrative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17be3cde-d4b9-476f-88a6-38dc94ec034e",
   "metadata": {},
   "source": [
    "### Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8647084d-0133-491a-be60-d92f965f266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc884a-d3c2-4bbe-a1cc-2fb0932fae91",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b8bd873-93cf-4df8-99e8-9af2a773e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efefaced-bcdf-4542-944a-9928df4ea96b",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "625512a0-95e6-40e8-a27f-44afbdda4c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>genre_names</th>\n",
       "      <th>original_language_full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1512553</th>\n",
       "      <td>29457</td>\n",
       "      <td>2176</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1112839937</td>\n",
       "      <td>The Glass House</td>\n",
       "      <td>2001-09-14</td>\n",
       "      <td>['Drama', 'Thriller']</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817397</th>\n",
       "      <td>35340</td>\n",
       "      <td>4470</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1069963591</td>\n",
       "      <td>Das Phantom</td>\n",
       "      <td>1994-09-05</td>\n",
       "      <td>['Crime', 'Documentary']</td>\n",
       "      <td>German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446383</th>\n",
       "      <td>8869</td>\n",
       "      <td>671</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1162869006</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>2001-11-16</td>\n",
       "      <td>['Adventure', 'Fantasy']</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612523</th>\n",
       "      <td>31361</td>\n",
       "      <td>41569</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1459354839</td>\n",
       "      <td>The Nomi Song</td>\n",
       "      <td>2004-10-14</td>\n",
       "      <td>['Documentary', 'Music']</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3616387</th>\n",
       "      <td>70300</td>\n",
       "      <td>1374</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1329612093</td>\n",
       "      <td>Rocky IV</td>\n",
       "      <td>1985-11-21</td>\n",
       "      <td>['Drama']</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  movie_id  rating   timestamp  \\\n",
       "1512553    29457      2176     3.5  1112839937   \n",
       "1817397    35340      4470     4.0  1069963591   \n",
       "446383      8869       671     5.0  1162869006   \n",
       "1612523    31361     41569     5.0  1459354839   \n",
       "3616387    70300      1374     0.5  1329612093   \n",
       "\n",
       "                                            title release_date  \\\n",
       "1512553                           The Glass House   2001-09-14   \n",
       "1817397                               Das Phantom   1994-09-05   \n",
       "446383   Harry Potter and the Philosopher's Stone   2001-11-16   \n",
       "1612523                             The Nomi Song   2004-10-14   \n",
       "3616387                                  Rocky IV   1985-11-21   \n",
       "\n",
       "                      genre_names original_language_full  \n",
       "1512553     ['Drama', 'Thriller']                English  \n",
       "1817397  ['Crime', 'Documentary']                 German  \n",
       "446383   ['Adventure', 'Fantasy']                English  \n",
       "1612523  ['Documentary', 'Music']                English  \n",
       "3616387                 ['Drama']                English  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Datasets/merged_moviecine_tmdb.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f365b60e-dd87-4589-816a-adce01fa3a2f",
   "metadata": {},
   "source": [
    "### Encoding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd5e0241-0119-44ea-a900-54c47a02e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_enc = LabelEncoder()\n",
    "movie_enc = LabelEncoder()\n",
    "\n",
    "df['user'] = user_enc.fit_transform(df['user_id'])\n",
    "df['movie'] = movie_enc.fit_transform(df['movie_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f712f462-753c-42c4-b0e2-46771eccd119",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = df['user'].nunique()\n",
    "num_movies = df['movie'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c5d27a9-e122-471f-893e-5b678aeaa7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200924"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f62e213d-cf97-485b-8f73-73890bf91900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11438"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99daf57e-e5b1-4eb5-8486-639f9ac4d0ca",
   "metadata": {},
   "source": [
    "### Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dffbd53d-6308-4134-ac2f-4f8791551c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df[['user', 'movie', 'rating']], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8298152-84df-474c-ba69-e9fb761acb70",
   "metadata": {},
   "source": [
    "### Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb67b515-cfea-4c9b-a216-8570382739dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRDataset(Dataset):\n",
    "    def __init__(self, df, num_items):\n",
    "        self.user_item_dict = df.groupby('user')['movie'].apply(set).to_dict()\n",
    "        self.users = df['user'].values\n",
    "        self.items = df['movie'].values\n",
    "        self.num_items = num_items\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.users[idx]\n",
    "        pos_item = self.items[idx]\n",
    "\n",
    "        while True:\n",
    "            neg_item = np.random.randint(0, self.num_items)\n",
    "            if neg_item not in self.user_item_dict.get(user, set()):\n",
    "                break\n",
    "\n",
    "        return torch.tensor(user), torch.tensor(pos_item), torch.tensor(neg_item)\n",
    "\n",
    "train_dataset = BPRDataset(train_df, num_movies)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23784db7-c936-42d1-b290-eaafba01978b",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63525eff-a75d-454d-b9ab-25a14128bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=64):\n",
    "        super(NCF, self).__init__()\n",
    "        self.user_embed_gmf = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embed_gmf = nn.Embedding(num_items, embedding_dim)\n",
    "        self.user_embed_mlp = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embed_mlp = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(embedding_dim + 1, 1)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        gmf_user = self.user_embed_gmf(user)\n",
    "        gmf_item = self.item_embed_gmf(item)\n",
    "        gmf_out = gmf_user * gmf_item\n",
    "\n",
    "        mlp_user = self.user_embed_mlp(user)\n",
    "        mlp_item = self.item_embed_mlp(item)\n",
    "        mlp_input = torch.cat([mlp_user, mlp_item], dim=-1)\n",
    "        mlp_out = self.mlp(mlp_input)\n",
    "\n",
    "        final_input = torch.cat([gmf_out, mlp_out], dim=-1)\n",
    "        prediction = self.output_layer(final_input)\n",
    "        return prediction.squeeze()\n",
    "\n",
    "def bpr_loss(pos_scores, neg_scores):\n",
    "    return -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7ad785-c67e-4555-b01f-25feb485db17",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c67539d3-cec8-4699-9da8-15118476aa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, BPR Loss: 0.1087, Val Loss: 6.8527, RMSE: 2.6178, MAE: 2.1071\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m     optimizer.zero_grad()\n\u001b[32m     34\u001b[39m     loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     total_loss += loss.item()\n\u001b[32m     37\u001b[39m train_losses.append(total_loss / \u001b[38;5;28mlen\u001b[39m(train_loader))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\optimizer.py:484\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    480\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    481\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    482\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    487\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\optimizer.py:89\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     88\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     91\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\adam.py:226\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    214\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    216\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    217\u001b[39m         group,\n\u001b[32m    218\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    223\u001b[39m         state_steps,\n\u001b[32m    224\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\optimizer.py:161\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\adam.py:766\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    764\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    785\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\optim\\adam.py:380\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[32m    379\u001b[39m exp_avg.lerp_(grad, \u001b[32m1\u001b[39m - beta1)\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m \u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[32m    383\u001b[39m     step = step_t\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NCF(num_users, num_movies, embedding_dim=64).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = torch.tensor(df['user'].values, dtype=torch.long)\n",
    "        self.items = torch.tensor(df['movie'].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(df['rating'].values, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.ratings[idx]\n",
    "\n",
    "test_dataset = RatingDataset(test_df)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience, wait = 3, 0\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for user, pos_item, neg_item in train_loader:\n",
    "        user, pos_item, neg_item = user.to(device), pos_item.to(device), neg_item.to(device)\n",
    "        pos_scores = model(user, pos_item)\n",
    "        neg_scores = model(user, neg_item)\n",
    "        loss = bpr_loss(pos_scores, neg_scores)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, all_preds, all_targets = 0, [], []\n",
    "    with torch.no_grad():\n",
    "        for users, items, ratings in test_loader:\n",
    "            users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
    "            preds = model(users, items)\n",
    "            loss = nn.MSELoss()(preds, ratings)\n",
    "            val_loss += loss.item()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(ratings.cpu().numpy())\n",
    "    val_loss /= len(test_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
    "    mae = mean_absolute_error(all_targets, all_preds)\n",
    "    print(f\"Epoch {epoch+1}, BPR Loss: {train_losses[-1]:.4f}, Val Loss: {val_loss:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        wait = 0\n",
    "        torch.save(model.state_dict(), \"best_ncf_model.pt\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70fa51-8930-4fe8-b6ae-f589ddf6947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "df['genre_names'] = df['genre_names'].fillna('').apply(eval)\n",
    "df['genre_text'] = df['genre_names'].apply(lambda genres: ' '.join(genres))\n",
    "df['combined'] = df['genre_text'] + ' ' + df['original_language_full']\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df['combined'])\n",
    "\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "movie_id_to_idx = dict(zip(df['movie_id'], df.index))\n",
    "idx_to_movie_id = dict(zip(df.index, df['movie_id']))\n",
    "\n",
    "def get_similar_movies(movie_id, top_k=10):\n",
    "    if movie_id not in movie_id_to_idx:\n",
    "        return []\n",
    "    idx = movie_id_to_idx[movie_id]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    top_indices = [i[0] for i in sim_scores[1:top_k+1]]\n",
    "    return [idx_to_movie_id[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5d250b-d17d-4a6a-b780-a60ef50e5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRecommender:\n",
    "    def __init__(self, model, user_enc, movie_enc, device):\n",
    "        self.model = model\n",
    "        self.user_enc = user_enc\n",
    "        self.movie_enc = movie_enc\n",
    "        self.device = device\n",
    "\n",
    "    def recommend(self, user_id, movie_id, top_k=10):\n",
    "        if user_id in self.user_enc.classes_ and movie_id in self.movie_enc.classes_:\n",
    "            try:\n",
    "                user_idx = self.user_enc.transform([user_id])[0]\n",
    "                user_tensor = torch.tensor([user_idx]*len(self.movie_enc.classes_)).to(self.device)\n",
    "                item_tensor = torch.arange(len(self.movie_enc.classes_)).to(self.device)\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    scores = self.model(user_tensor, item_tensor).cpu().numpy()\n",
    "                top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "                return self.movie_enc.inverse_transform(top_indices)\n",
    "            except:\n",
    "                pass\n",
    "        print(\"⚠️ Cold-start: Using content-based fallback\")\n",
    "        return get_similar_movies(movie_id, top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63d54d-4340-4031-8c2a-b797776bfe02",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b48f0e-8904-4d9d-a710-cd51842b0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topk_metrics(model, test_df, all_items, k=10, device='cpu'):\n",
    "    model.eval()\n",
    "    hits, ndcgs, precisions, recalls = [], [], [], []\n",
    "    users = test_df['user'].unique()\n",
    "    sampled_users = np.random.choice(users, size=min(1000, len(users)), replace=False)\n",
    "\n",
    "    for user in sampled_users:\n",
    "        user_movies = set(test_df[test_df['user'] == user]['movie'])\n",
    "        if not user_movies:\n",
    "            continue\n",
    "        true_item = list(user_movies)[0]\n",
    "        negatives = list(all_items - user_movies)\n",
    "        if len(negatives) < 99:\n",
    "            continue\n",
    "        sampled_negatives = np.random.choice(negatives, 99, replace=False).tolist()\n",
    "        test_items = [true_item] + sampled_negatives\n",
    "        user_tensor = torch.tensor([user] * len(test_items)).to(device)\n",
    "        item_tensor = torch.tensor(test_items).to(device)\n",
    "        with torch.no_grad():\n",
    "            scores = model(user_tensor, item_tensor).squeeze().cpu().numpy()\n",
    "        ranked_items = np.array(test_items)[np.argsort(-scores)][:k]\n",
    "        hit = int(true_item in ranked_items)\n",
    "        hits.append(hit)\n",
    "        precisions.append(hit / k)\n",
    "        recalls.append(hit / 1)\n",
    "        ndcgs.append(1 / np.log2(np.where(ranked_items == true_item)[0][0] + 2) if hit else 0)\n",
    "\n",
    "    return {\n",
    "        \"HitRate@K\": np.mean(hits),\n",
    "        \"Precision@K\": np.mean(precisions),\n",
    "        \"Recall@K\": np.mean(recalls),\n",
    "        \"NDCG@K\": np.mean(ndcgs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e494700-3dfb-4233-8ff4-417f6cdd165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = set(df['movie'].unique())\n",
    "test_df_sample = df.groupby('user').first().reset_index()[['user', 'movie']]\n",
    "metrics = evaluate_topk_metrics(model, test_df_sample, all_items, k=10, device=device)\n",
    "\n",
    "print(\"\\n🎯 Top-K Evaluation:\")\n",
    "print(f\"HitRate@10  : {metrics['HitRate@K']:.4f}\")\n",
    "print(f\"Precision@10: {metrics['Precision@K']:.4f}\")\n",
    "print(f\"Recall@10   : {metrics['Recall@K']:.4f}\")\n",
    "print(f\"NDCG@10     : {metrics['NDCG@K']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb7c397-945b-4aa1-bba0-b4f0bb6ce31d",
   "metadata": {},
   "source": [
    "### Output Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea1d65-7431-4485-ae02-95aa3d0eef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee5e9f4-76b2-4ed3-b9e6-158427988bf7",
   "metadata": {},
   "source": [
    "### Dumping the model and encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa36fd-45d5-4e3d-a9ce-f625692a4ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"best_ncf_model.pt\"))\n",
    "with open(\"movie_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(movie_enc, f)\n",
    "with open(\"user_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(user_enc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c72e4b9-24d0-4a39-a72e-82e632d79a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
