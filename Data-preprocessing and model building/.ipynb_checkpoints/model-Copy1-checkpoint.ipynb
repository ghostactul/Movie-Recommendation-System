{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77c31c0d-3a18-44eb-bdf9-0a049495aa87",
   "metadata": {},
   "source": [
    "# Neural Collabrative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17be3cde-d4b9-476f-88a6-38dc94ec034e",
   "metadata": {},
   "source": [
    "### Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8647084d-0133-491a-be60-d92f965f266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc884a-d3c2-4bbe-a1cc-2fb0932fae91",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b8bd873-93cf-4df8-99e8-9af2a773e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efefaced-bcdf-4542-944a-9928df4ea96b",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "625512a0-95e6-40e8-a27f-44afbdda4c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>genre_names</th>\n",
       "      <th>original_language_full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8789330</th>\n",
       "      <td>171636</td>\n",
       "      <td>4638</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1674530557</td>\n",
       "      <td>Hot Fuzz</td>\n",
       "      <td>2007-02-14</td>\n",
       "      <td>['Crime', 'Action', 'Comedy']</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4343493</th>\n",
       "      <td>84433</td>\n",
       "      <td>4368</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1005626863</td>\n",
       "      <td>Dark Wolf</td>\n",
       "      <td>2003-04-15</td>\n",
       "      <td>['Horror']</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3148319</th>\n",
       "      <td>61157</td>\n",
       "      <td>8665</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1620217004</td>\n",
       "      <td>K-19: The Widowmaker</td>\n",
       "      <td>2002-07-19</td>\n",
       "      <td>['Drama', 'History', 'Thriller', 'Mystery', 'W...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7706255</th>\n",
       "      <td>150309</td>\n",
       "      <td>31225</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1155505369</td>\n",
       "      <td>Paris Is Burning</td>\n",
       "      <td>1991-03-13</td>\n",
       "      <td>['Documentary']</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786627</th>\n",
       "      <td>34729</td>\n",
       "      <td>2132</td>\n",
       "      <td>4.0</td>\n",
       "      <td>951554777</td>\n",
       "      <td>Totally Blonde</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>['Comedy']</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  movie_id  rating   timestamp                 title  \\\n",
       "8789330   171636      4638     2.0  1674530557              Hot Fuzz   \n",
       "4343493    84433      4368     2.0  1005626863             Dark Wolf   \n",
       "3148319    61157      8665     5.0  1620217004  K-19: The Widowmaker   \n",
       "7706255   150309     31225     3.0  1155505369      Paris Is Burning   \n",
       "1786627    34729      2132     4.0   951554777        Totally Blonde   \n",
       "\n",
       "        release_date                                        genre_names  \\\n",
       "8789330   2007-02-14                      ['Crime', 'Action', 'Comedy']   \n",
       "4343493   2003-04-15                                         ['Horror']   \n",
       "3148319   2002-07-19  ['Drama', 'History', 'Thriller', 'Mystery', 'W...   \n",
       "7706255   1991-03-13                                    ['Documentary']   \n",
       "1786627   2001-01-01                                         ['Comedy']   \n",
       "\n",
       "        original_language_full  \n",
       "8789330                English  \n",
       "4343493                English  \n",
       "3148319                English  \n",
       "7706255                English  \n",
       "1786627                English  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Datasets/merged_moviecine_tmdb.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f365b60e-dd87-4589-816a-adce01fa3a2f",
   "metadata": {},
   "source": [
    "### Encoding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd5e0241-0119-44ea-a900-54c47a02e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_enc = LabelEncoder()\n",
    "movie_enc = LabelEncoder()\n",
    "\n",
    "df['user'] = user_enc.fit_transform(df['user_id'])\n",
    "df['movie'] = movie_enc.fit_transform(df['movie_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f712f462-753c-42c4-b0e2-46771eccd119",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = df['user'].nunique()\n",
    "num_movies = df['movie'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c5d27a9-e122-471f-893e-5b678aeaa7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200924"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f62e213d-cf97-485b-8f73-73890bf91900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11438"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99daf57e-e5b1-4eb5-8486-639f9ac4d0ca",
   "metadata": {},
   "source": [
    "### Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dffbd53d-6308-4134-ac2f-4f8791551c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df[['user', 'movie', 'rating']], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8298152-84df-474c-ba69-e9fb761acb70",
   "metadata": {},
   "source": [
    "### Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb67b515-cfea-4c9b-a216-8570382739dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     19\u001b[39m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     21\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch.tensor(user, dtype=torch.long), \\\n\u001b[32m     22\u001b[39m                torch.tensor(pos_item, dtype=torch.long), \\\n\u001b[32m     23\u001b[39m                torch.tensor(neg_item, dtype=torch.long)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m train_dataset = \u001b[43mBPRDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_movies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m train_loader = DataLoader(train_dataset, batch_size=\u001b[32m1024\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mBPRDataset.__init__\u001b[39m\u001b[34m(self, df, num_items)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, df, num_items):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28mself\u001b[39m.user_item_dict = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmovie\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m)\u001b[49m.to_dict()\n\u001b[32m      4\u001b[39m     \u001b[38;5;28mself\u001b[39m.users = df[\u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m].values\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.items = df[\u001b[33m'\u001b[39m\u001b[33mmovie\u001b[39m\u001b[33m'\u001b[39m].values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\groupby\\generic.py:230\u001b[39m, in \u001b[36mSeriesGroupBy.apply\u001b[39m\u001b[34m(self, func, *args, **kwargs)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(\n\u001b[32m    225\u001b[39m     _apply_docs[\u001b[33m\"\u001b[39m\u001b[33mtemplate\u001b[39m\u001b[33m\"\u001b[39m].format(\n\u001b[32m    226\u001b[39m         \u001b[38;5;28minput\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mseries\u001b[39m\u001b[33m\"\u001b[39m, examples=_apply_docs[\u001b[33m\"\u001b[39m\u001b[33mseries_examples\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    227\u001b[39m     )\n\u001b[32m    228\u001b[39m )\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, *args, **kwargs) -> Series:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\groupby\\groupby.py:1824\u001b[39m, in \u001b[36mGroupBy.apply\u001b[39m\u001b[34m(self, func, include_groups, *args, **kwargs)\u001b[39m\n\u001b[32m   1822\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1823\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1824\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1825\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1826\u001b[39m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj, Series)\n\u001b[32m   1827\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1828\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selected_obj.shape != \u001b[38;5;28mself\u001b[39m._obj_with_exclusions.shape\n\u001b[32m   1829\u001b[39m         ):\n\u001b[32m   1830\u001b[39m             warnings.warn(\n\u001b[32m   1831\u001b[39m                 message=_apply_groupings_depr.format(\n\u001b[32m   1832\u001b[39m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1835\u001b[39m                 stacklevel=find_stack_level(),\n\u001b[32m   1836\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\groupby\\groupby.py:1885\u001b[39m, in \u001b[36mGroupBy._python_apply_general\u001b[39m\u001b[34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[39m\n\u001b[32m   1850\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   1851\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_python_apply_general\u001b[39m(\n\u001b[32m   1852\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1857\u001b[39m     is_agg: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1858\u001b[39m ) -> NDFrameT:\n\u001b[32m   1859\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1860\u001b[39m \u001b[33;03m    Apply function f in python space\u001b[39;00m\n\u001b[32m   1861\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1883\u001b[39m \u001b[33;03m        data after applying f\u001b[39;00m\n\u001b[32m   1884\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1885\u001b[39m     values, mutated = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_grouper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1886\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1887\u001b[39m         not_indexed_same = mutated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\groupby\\ops.py:922\u001b[39m, in \u001b[36mBaseGrouper.apply_groupwise\u001b[39m\u001b[34m(self, f, data, axis)\u001b[39m\n\u001b[32m    920\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[32m    921\u001b[39m         mutated = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     result_values.append(res)\n\u001b[32m    923\u001b[39m \u001b[38;5;66;03m# getattr pattern for __name__ is needed for functools.partial objects\u001b[39;00m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(group_keys) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(f, \u001b[33m\"\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m    925\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mskew\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    926\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msum\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    930\u001b[39m     \u001b[38;5;66;03m#  so we will not have raised even if this is an invalid dtype.\u001b[39;00m\n\u001b[32m    931\u001b[39m     \u001b[38;5;66;03m#  So do one dummy call here to raise appropriate TypeError.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "class BPRDataset(Dataset):\n",
    "    def __init__(self, df, num_items):\n",
    "        self.user_item_dict = df.groupby('user')['movie'].apply(set).to_dict()\n",
    "        self.users = df['user'].values\n",
    "        self.items = df['movie'].values\n",
    "        self.num_items = num_items\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.users[idx]\n",
    "        pos_item = self.items[idx]\n",
    "\n",
    "        # Sample a negative item\n",
    "        while True:\n",
    "            neg_item = np.random.randint(0, self.num_items)\n",
    "            if neg_item not in self.user_item_dict.get(user, set()):\n",
    "                break\n",
    "\n",
    "        return torch.tensor(user, dtype=torch.long), \\\n",
    "               torch.tensor(pos_item, dtype=torch.long), \\\n",
    "               torch.tensor(neg_item, dtype=torch.long)\n",
    "               \n",
    "train_dataset = BPRDataset(train_df, num_movies)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23784db7-c936-42d1-b290-eaafba01978b",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63525eff-a75d-454d-b9ab-25a14128bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=32):\n",
    "        super(NCF, self).__init__()\n",
    "        \n",
    "        # GMF part\n",
    "        self.user_embed_gmf = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embed_gmf = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        # MLP part\n",
    "        self.user_embed_mlp = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embed_mlp = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "        # Final layer\n",
    "        self.output_layer = nn.Linear(embedding_dim + 1, 1)  # 1 for the MLP output (single prediction)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        # GMF part\n",
    "        gmf_user = self.user_embed_gmf(user)\n",
    "        gmf_item = self.item_embed_gmf(item)\n",
    "        gmf_out = gmf_user * gmf_item  # Element-wise multiplication for GMF\n",
    "\n",
    "        # MLP part\n",
    "        mlp_user = self.user_embed_mlp(user)\n",
    "        mlp_item = self.item_embed_mlp(item)\n",
    "        mlp_input = torch.cat([mlp_user, mlp_item], dim=-1)\n",
    "        mlp_out = self.mlp(mlp_input)  # MLP outputs a single value for each user-item pair\n",
    "\n",
    "        # Concatenate GMF + MLP outputs\n",
    "        final_input = torch.cat([gmf_out, mlp_out], dim=-1)  # Concatenate the outputs\n",
    "        prediction = self.output_layer(final_input)  # Final layer to produce the predicted rating\n",
    "        return prediction.squeeze()  # Return the final prediction\n",
    "    \n",
    "    # Define BPR loss function\n",
    "def bpr_loss(pos_scores, neg_scores):\n",
    "    return -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7ad785-c67e-4555-b01f-25feb485db17",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67539d3-cec8-4699-9da8-15118476aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NCF(num_users, num_movies).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_losses = []\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for user, pos_item, neg_item in train_loader:\n",
    "        user = user.to(device)\n",
    "        pos_item = pos_item.to(device)\n",
    "        neg_item = neg_item.to(device)\n",
    "\n",
    "        pos_scores = model(user, pos_item)\n",
    "        neg_scores = model(user, neg_item)\n",
    "\n",
    "        loss = bpr_loss(pos_scores, neg_scores)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, BPR Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63d54d-4340-4031-8c2a-b797776bfe02",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86416dac-ed84-444c-830b-2bafa74ba464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "class RatingDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = torch.tensor(df['user'].values, dtype=torch.long)\n",
    "        self.items = torch.tensor(df['movie'].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(df['rating'].values, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.ratings[idx]\n",
    "test_dataset = RatingDataset(test_df)\n",
    "# Create DataLoader for testing\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "loss_fn1 = nn.MSELoss()\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "val_losses = []\n",
    "val_loss = 0  # Initialize val_loss\n",
    "\n",
    "with torch.no_grad():\n",
    "    for users, movies, ratings in test_loader:\n",
    "        users, movies, ratings = users.to(device), movies.to(device), ratings.to(device)\n",
    "\n",
    "        preds = model(users, movies)\n",
    "        preds = preds.view(-1)\n",
    "        ratings = ratings.view(-1)\n",
    "\n",
    "        loss = loss_fn1(preds, ratings)\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(ratings.cpu().numpy())\n",
    "\n",
    "val_loss /= len(test_loader)\n",
    "val_losses.append(val_loss)\n",
    "\n",
    "# Compute RMSE and MAE\n",
    "rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
    "mae = mean_absolute_error(all_targets, all_preds)\n",
    "\n",
    "print(f\"\\n📊 Evaluation Results:\")\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE : {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b48f0e-8904-4d9d-a710-cd51842b0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def evaluate_topk_metrics(model, test_df, all_items, k=10, num_users=1000, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate HitRate@K, Precision@K, and NDCG@K.\n",
    "\n",
    "    model: trained PyTorch NCF model\n",
    "    test_df: DataFrame with columns ['user', 'movie']\n",
    "    all_items: set of all item IDs\n",
    "    k: top-k items to consider\n",
    "    num_users: number of users to evaluate\n",
    "    device: 'cuda' or 'cpu'\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    hits, ndcgs, precisions = [], [], []\n",
    "\n",
    "    users = test_df['user'].unique()\n",
    "    sampled_users = np.random.choice(users, min(num_users, len(users)), replace=False)\n",
    "\n",
    "    for user in sampled_users:\n",
    "        true_item = test_df[test_df['user'] == user]['movie'].values[0]\n",
    "\n",
    "        # Sample 99 negative items\n",
    "        negatives = list(all_items - set(test_df[test_df['user'] == user]['movie']))\n",
    "        if len(negatives) < 99:\n",
    "            continue  # Skip if not enough negatives\n",
    "        sampled_negatives = np.random.choice(negatives, size=99, replace=False).tolist()\n",
    "\n",
    "        # Combine true item with negatives\n",
    "        test_items = [true_item] + sampled_negatives\n",
    "        user_tensor = torch.tensor([user] * len(test_items)).to(device)\n",
    "        item_tensor = torch.tensor(test_items).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = model(user_tensor, item_tensor).squeeze().cpu().numpy()\n",
    "\n",
    "        ranked_indices = np.argsort(-scores)\n",
    "        ranked_items = np.array(test_items)[ranked_indices]\n",
    "\n",
    "        top_k = ranked_items[:k]\n",
    "        hit = int(true_item in top_k)\n",
    "        hits.append(hit)\n",
    "\n",
    "        if hit:\n",
    "            rank_position = np.where(top_k == true_item)[0][0] + 1\n",
    "            ndcgs.append(1 / np.log2(rank_position + 1))\n",
    "        else:\n",
    "            ndcgs.append(0)\n",
    "\n",
    "        precisions.append(hit / k)\n",
    "\n",
    "    return {\n",
    "        \"HitRate@K\": np.mean(hits),\n",
    "        \"Precision@K\": np.mean(precisions),\n",
    "        \"NDCG@K\": np.mean(ndcgs)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e494700-3dfb-4233-8ff4-417f6cdd165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items = set(df['movie'].unique())\n",
    "\n",
    "# Step 2: Create a test DataFrame with user-movie pairs\n",
    "# For demo, we take one rating per user (you can improve this later)\n",
    "test_df = df.groupby('user').first().reset_index()[['user', 'movie']]\n",
    "\n",
    "metrics = evaluate_topk_metrics(model, test_df, all_items, k=10, device=device)\n",
    "print(f\"🎯 Evaluation Results:\")\n",
    "print(f\"HitRate@10  : {metrics['HitRate@K']:.4f}\")\n",
    "print(f\"Precision@10: {metrics['Precision@K']:.4f}\")\n",
    "print(f\"NDCG@10     : {metrics['NDCG@K']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb7c397-945b-4aa1-bba0-b4f0bb6ce31d",
   "metadata": {},
   "source": [
    "### Output Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea1d65-7431-4485-ae02-95aa3d0eef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee5e9f4-76b2-4ed3-b9e6-158427988bf7",
   "metadata": {},
   "source": [
    "### Dumping the model and encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa36fd-45d5-4e3d-a9ce-f625692a4ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "torch.save(model.state_dict(), \"ncf_model.pth\")\n",
    "\n",
    "with open(\"movie_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(movie_enc, f)\n",
    "with open(\"user_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(user_enc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5bf4f3-c8dd-4761-8b69-d55d87eea3e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
